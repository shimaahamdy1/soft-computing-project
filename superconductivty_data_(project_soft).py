# -*- coding: utf-8 -*-
"""Superconductivty Data (project soft).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pxazg6LHkW1BdKBhBp7PXlKEI84Q15bY
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Soft computing/Superconductivty Data.csv")

data.info()

data.isnull().sum()

data.head(10)

"""**SVM**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.feature_selection import SelectKBest, f_regression

SEED = 30

# 1. تحميل البيانات
data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Soft computing/Superconductivty Data.csv')

# 2. فصل الميزات والهدف
X = data.drop('critical_temp', axis=1)
y = data['critical_temp']

# 3. تقسيم البيانات
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)

# 4. تطبيع البيانات (قبل اختيار الميزات)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 5. تدريب SVR قبل اختيار الميزات
model = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)
model.fit(X_train_scaled, y_train)
y_pred_before = model.predict(X_test_scaled)

# 6. اختيار أفضل 5 ميزات
selector = SelectKBest(score_func=f_regression, k=5)
X_train_selected = selector.fit_transform(X_train, y_train)
X_test_selected = selector.transform(X_test)

# 7. تطبيع البيانات بعد اختيار الميزات
scaler_selected = StandardScaler()
X_train_selected_scaled = scaler_selected.fit_transform(X_train_selected)
X_test_selected_scaled = scaler_selected.transform(X_test_selected)

# 8. تدريب SVR بعد اختيار الميزات
model_selected = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)
model_selected.fit(X_train_selected_scaled, y_train)
y_pred_after = model_selected.predict(X_test_selected_scaled)

# 9. حساب المقاييس
def get_metrics(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    return mse, rmse, mae, r2

def accuracy_like(y_true, y_pred, tolerance=5):
    diff = np.abs(y_true - y_pred)
    within_range = np.sum(diff <= tolerance)
    return within_range / len(y_true) * 100

mse_before, rmse_before, mae_before, r2_before = get_metrics(y_test, y_pred_before)
mse_after, rmse_after, mae_after, r2_after = get_metrics(y_test, y_pred_after)

acc_before = accuracy_like(y_test, y_pred_before)
acc_after = accuracy_like(y_test, y_pred_after)

# 10. طباعة القيم
print("Performance Metrics:")
print("\nBefore Feature Selection:")
print(f"  MSE: {mse_before:.3f}")
print(f"  RMSE: {rmse_before:.3f}")
print(f"  MAE: {mae_before:.3f}")
print(f"  R2: {r2_before:.3f}")
print(f"  Accuracy-like (±5): {acc_before:.2f}%")

print("\nAfter Feature Selection:")
print(f"  MSE: {mse_after:.3f}")
print(f"  RMSE: {rmse_after:.3f}")
print(f"  MAE: {mae_after:.3f}")
print(f"  R2: {r2_after:.3f}")
print(f"  Accuracy-like (±5): {acc_after:.2f}%")

# 11. Bar Chart للمقارنة
metrics_names = ['MSE', 'RMSE', 'MAE', 'R2', 'Accuracy-like']
before_values = [mse_before, rmse_before, mae_before, r2_before, acc_before]
after_values = [mse_after, rmse_after, mae_after, r2_after, acc_after]

x = np.arange(len(metrics_names))
width = 0.35

fig, ax = plt.subplots(figsize=(12, 6))
bars1 = ax.bar(x - width/2, before_values, width, label='Before Feature Selection', color='skyblue')
bars2 = ax.bar(x + width/2, after_values, width, label='After Feature Selection', color='orange')

ax.set_ylabel('Scores')
ax.set_title('🔬 SVM Performance Before vs After Feature Selection')
ax.set_xticks(x)
ax.set_xticklabels(metrics_names)
ax.legend()
ax.grid(True, axis='y')

# عرض القيم فوق الأعمدة
def autolabel(bars):
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.3f}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),
                    textcoords='offset points',
                    ha='center', va='bottom')

autolabel(bars1)
autolabel(bars2)

plt.tight_layout()
plt.show()

# 12. طباعة ملخص الـ Accuracy-like بشكل واضح
print("="*60)
print("Summary of Accuracy-like (±5):")
print(f"Before Feature Selection: {acc_before:.2f}%")
print(f"After Feature Selection:  {acc_after:.2f}%")
print("="*60)

"""**Random** **forest**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# 1. تحميل البيانات
data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Soft computing/Superconductivty Data.csv')

# 2. فصل الميزات والهدف
X = data.drop('critical_temp', axis=1)
y = data['critical_temp']

# === دوال الحساب ===
def get_metrics(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    acc_like = np.mean(np.abs(y_true - y_pred) <= 5) * 100
    return [mse, rmse, mae, r2, acc_like]

# === تدريب النموذج قبل اختيار الميزات ===
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train_scaled, y_train)
y_pred_before = model.predict(X_test_scaled)
metrics_before = get_metrics(y_test, y_pred_before)

# === تدريب النموذج بعد اختيار الميزات (أول 50 ميزة) ===
selected_features = X.columns[:50]
X_sel = X[selected_features]

X_train_sel, X_test_sel, y_train_sel, y_test_sel = train_test_split(X_sel, y, test_size=0.2, random_state=42)
X_train_sel_scaled = scaler.fit_transform(X_train_sel)
X_test_sel_scaled = scaler.transform(X_test_sel)

model_sel = RandomForestRegressor(n_estimators=100, random_state=42)
model_sel.fit(X_train_sel_scaled, y_train_sel)
y_pred_after = model_sel.predict(X_test_sel_scaled)
metrics_after = get_metrics(y_test_sel, y_pred_after)

# === طباعة القيم النصية ===
metrics_names = ['MSE', 'RMSE', 'MAE', 'R²', 'Accuracy-like']
print("\n Performance Metrics Comparison:\n")

print("Before Feature Selection:")
for name, value in zip(metrics_names, metrics_before):
    print(f"  {name}: {value:.3f}")

print("\nAfter Feature Selection:")
for name, value in zip(metrics_names, metrics_after):
    print(f"  {name}: {value:.3f}")

# === رسم Bar Chart للمقارنة ===
x = np.arange(len(metrics_names))
width = 0.35

fig, ax = plt.subplots(figsize=(12, 6))
bars1 = ax.bar(x - width/2, metrics_before, width, label='Before Feature Selection', color='salmon')
bars2 = ax.bar(x + width/2, metrics_after, width, label='After Feature Selection', color='skyblue')

ax.set_ylabel('Scores')
ax.set_title('Comparison of Model Performance Before and After Feature Selection')
ax.set_xticks(x)
ax.set_xticklabels(metrics_names)
ax.legend()
ax.grid(True, axis='y')

# وضع القيم فوق الأعمدة
def autolabel(bars):
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.3f}',
                    xy=(bar.get_x() + bar.get_width()/2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha='center', va='bottom')

autolabel(bars1)
autolabel(bars2)

plt.tight_layout()
plt.show()

"""**KNN**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# 1. تحميل البيانات
data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Soft computing/Superconductivty Data.csv')

# 2. فصل الميزات والهدف
X = data.drop('critical_temp', axis=1)
y = data['critical_temp']

# === دالة لحساب المقاييس ===
def get_metrics(y_true, y_pred, tolerance=5):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    acc_like = np.mean(np.abs(y_true - y_pred) <= tolerance) * 100
    return [r2, mae, mse, rmse, acc_like]

# === النموذج قبل اختيار الميزات ===
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

knn_before = KNeighborsRegressor(n_neighbors=5)
knn_before.fit(X_train_scaled, y_train)
y_pred_before = knn_before.predict(X_test_scaled)
metrics_before = get_metrics(y_test, y_pred_before)

# === النموذج بعد اختيار الميزات (مثلاً أول 50 ميزة) ===
selected_features = X.columns[:50]
X_sel = X[selected_features]

X_train_sel, X_test_sel, y_train_sel, y_test_sel = train_test_split(X_sel, y, test_size=0.2, random_state=42)
X_train_sel_scaled = scaler.fit_transform(X_train_sel)
X_test_sel_scaled = scaler.transform(X_test_sel)

knn_after = KNeighborsRegressor(n_neighbors=5)
knn_after.fit(X_train_sel_scaled, y_train_sel)
y_pred_after = knn_after.predict(X_test_sel_scaled)
metrics_after = get_metrics(y_test_sel, y_pred_after)

# === طباعة النتائج ===
metrics_names = ['R²', 'MAE', 'MSE', 'RMSE', 'Accuracy-like (%)']

print("\nKNN Performance Comparison:\n")
print("Before Feature Selection:")
for name, value in zip(metrics_names, metrics_before):
    print(f"  {name}: {value:.3f}")

print("\nAfter Feature Selection:")
for name, value in zip(metrics_names, metrics_after):
    print(f"  {name}: {value:.3f}")

# === Bar Chart للمقارنة ===
x = np.arange(len(metrics_names))
width = 0.35

fig, ax = plt.subplots(figsize=(14, 6))
bars1 = ax.bar(x - width/2, metrics_before, width, label='Before Feature Selection', color='coral')
bars2 = ax.bar(x + width/2, metrics_after, width, label='After Feature Selection', color='mediumseagreen')

ax.set_ylabel('Score')
ax.set_title('KNN Model Performance: Before vs After Feature Selection')
ax.set_xticks(x)
ax.set_xticklabels(metrics_names)
ax.legend()
ax.grid(True, axis='y')

# دالة لعرض القيم فوق الأعمدة
def autolabel(bars):
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.2f}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha='center', va='bottom')

autolabel(bars1)
autolabel(bars2)

plt.tight_layout()
plt.show()

"""**GA**"""

# Importing dependencies
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler # Use StandardScaler consistently
# Import regression metrics
from sklearn.metrics import mean_squared_error, r2_score
# Import a regressor model, like SVR or RandomForestRegressor, consistent with previous sections
from sklearn.svm import SVR # Using SVR as an example, based on a previous section
from sklearn.metrics import mean_absolute_error # Import MAE for final evaluation

SEED=30

# --- Data Loading and Preparation (moved here) ---
# 1. تحميل البيانات
data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Soft computing/Superconductivty Data.csv')

# 2. فصل الميزات والهدف
X = data.drop('critical_temp', axis=1)
y = data['critical_temp']

# 3. تقسيم Train / Test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. تطبيع البيانات
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
# --- End Data Loading and Preparation ---


# Define GA hyperparameters
size = 10  # Population size
# Use the scaled data's shape for n_feat
n_feat = X_train_scaled.shape[1]  # Number of features
n_parents = 15  # Number of parents to select
mutation_rate = 0.1  # Mutation rate
n_gen = 5  # Number of generations

# Function for initialization of population in GA
def initialization_of_population(size, n_feat):
    population = []
    for i in range(size):
        chromosome = np.ones(n_feat, dtype=bool)
        chromosome[:int(0.3 * n_feat)] = False
        np.random.shuffle(chromosome)
        population.append(chromosome)
    return population

# Modified fitness function to accept the model and use scaled data
def fitness_score(population, model_instance, X_train_scaled, X_test_scaled, y_train, y_test):
    scores = []
    for chromosome in population:
        # Select features using boolean indexing on NumPy arrays (scaled data)
        # Ensure chromosome is a boolean numpy array
        selected_features = chromosome.astype(bool)

        X_train_selected = X_train_scaled[:, selected_features]
        X_test_selected = X_test_scaled[:, selected_features]

        # Ensure selected features have columns before fitting
        if X_train_selected.shape[1] == 0 or X_test_selected.shape[1] == 0:
             scores.append(-1e9) # Assign a very low score if no features are selected (R² won't be calculated)
             continue

        # Fit the model on the selected features
        model_instance.fit(X_train_selected, y_train)
        predictions = model_instance.predict(X_test_selected)

        # Calculate fitness using R² score (maximize R², or minimize 1-R²)
        r2 = r2_score(y_test, predictions)
        scores.append(r2)

    scores, population = np.array(scores), np.array(population)
    # Sort by score (descending for R²)
    inds = np.argsort(scores)[::-1]
    return list(scores[inds]), list(population[inds, :])

# Function for selection in GA
def selection(pop_after_fit, n_parents):
    # Take the top n_parents from the sorted population
    return pop_after_fit[:n_parents]

# Function for crossover in GA
def crossover(pop_after_sel, target_size):
    pop_nextgen = list(pop_after_sel) # Start with selected parents
    current_size = len(pop_nextgen)

    # Crossover until we reach or exceed the target population size
    while current_size < target_size:
        # Select two parents randomly (with replacement is fine for simplicity)
        parent_indices = np.random.choice(len(pop_after_sel), 2, replace=False) # Use replace=False to avoid same parent twice
        parent1 = pop_after_sel[parent_indices[0]]
        parent2 = pop_after_sel[parent_indices[1]]

        # Ensure they are numpy arrays for concatenation
        parent1 = np.array(parent1)
        parent2 = np.array(parent2)

        # Perform crossover at a random point
        crossover_point = np.random.randint(1, len(parent1)) # Point must be between 1 and n_feat-1

        child_1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))
        child_2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))

        # Add children to the next generation pool
        pop_nextgen.extend([child_1, child_2])
        current_size += 2

    return pop_nextgen[:target_size] # Return only up to the target size


# Function for mutation in GA
def mutation(pop_after_cross, mutation_rate, n_feat):
    pop_next_gen = []
    # Calculate number of bits to flip per chromosome based on mutation_rate
    # Ensure at least one flip if mutation rate is > 0
    mutation_count_per_chromo = int(mutation_rate * n_feat)
    if mutation_count_per_chromo == 0 and mutation_rate > 0:
        mutation_count_per_chromo = 1


    for chromo in pop_after_cross:
        # Make a copy to avoid modifying in place
        mutated_chromo = chromo.copy()
        if mutation_count_per_chromo > 0:
            # Select random positions to mutate without replacement
            rand_posi = np.random.choice(n_feat, mutation_count_per_chromo, replace=False)
            # Flip the bits at the selected positions
            mutated_chromo[rand_posi] = ~mutated_chromo[rand_posi]
        pop_next_gen.append(mutated_chromo)
    return pop_next_gen

# Modified generations function to accept the model and scaled data
def generations(size, n_feat, n_parents, mutation_rate, n_gen, X_train_scaled, X_test_scaled, y_train, y_test, model_instance):
    best_chromo = []
    best_score = []
    population = initialization_of_population(size, n_feat)
    for i in range(n_gen):
        # Pass scaled data and model instance to fitness_score
        scores, pop_after_fit = fitness_score(population, model_instance, X_train_scaled, X_test_scaled, y_train, y_test)

        # Handle potential issue if all scores are -inf (e.g., no features selected in any chromosome)
        if scores and scores[0] <= -1e8: # Check if the best score is the penalty score
             print(f'Generation {i + 1}: No valid chromosomes (R² cannot be calculated).')
             # Option: Initialize a new population or skip this generation/stop early
             population = initialization_of_population(size, n_feat) # Re-initialize to try again
             continue # Skip to next generation

        print(f'Best R² score in generation {i + 1}: {scores[0]:.3f}') # Print best R² score

        pop_after_sel = selection(pop_after_fit, n_parents)
        pop_after_cross = crossover(pop_after_sel, size) # Crossover to produce `size` individuals
        population = mutation(pop_after_cross, mutation_rate, n_feat)

        # Ensure population size is exactly 'size' if mutation/crossover doesn't guarantee it
        # (The current crossover implementation targets 'size', mutation doesn't change it)
        # population = population[:size] # This line might be redundant depending on mutation implementation

        # Append the best chromosome and score of THIS generation (before potential re-initialization)
        best_chromo.append(pop_after_fit[0])
        best_score.append(scores[0])

    return best_chromo, best_score

# --- Code to run the GA ---

# Define the model instance (choose a regressor)
# Let's use SVR with some parameters as used in a previous cell
model_for_ga = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)
# If you prefer Random Forest:
# from sklearn.ensemble import RandomForestRegressor
# model_for_ga = RandomForestRegressor(n_estimators=100, random_state=42)
# Or KNN:
# from sklearn.neighbors import KNeighborsRegressor
# model_for_ga = KNeighborsRegressor(n_neighbors=5)


# Run GA with the scaled data and model instance
# Ensure X_train_scaled, X_test_scaled, y_train, y_test are defined from the data loading steps above
best_chromos_per_gen, best_scores_per_gen = generations(size, n_feat, n_parents, mutation_rate, n_gen,
                                       X_train_scaled, X_test_scaled, y_train, y_test, model_for_ga)

print("\nGA completed.")

# Find the overall best score and corresponding chromosome across all generations
# Filter out penalty scores if any
valid_scores_indices = [i for i, score in enumerate(best_scores_per_gen) if score > -1e8]

if not valid_scores_indices:
    print("GA failed to find any valid chromosomes with calculated R² scores.")
else:
    overall_best_score_idx = valid_scores_indices[np.argmax([best_scores_per_gen[i] for i in valid_scores_indices])]
    overall_best_score = best_scores_per_gen[overall_best_score_idx]
    best_final_chromosome = best_chromos_per_gen[overall_best_score_idx]

    print(f"Overall best R² score found: {overall_best_score:.3f}")
    print(f"Best feature mask (chromosome): {best_final_chromosome}")
    print(f"Number of features selected: {np.sum(best_final_chromosome)}")

    # Optional: Train the final model on the best selected features
    # Check if the best chromosome selected any features
    if np.sum(best_final_chromosome) == 0:
        print("\nBest GA chromosome selected 0 features. Cannot train final model on selected features.")
    else:
        X_train_optimal = X_train_scaled[:, best_final_chromosome.astype(bool)]
        X_test_optimal = X_test_scaled[:, best_final_chromosome.astype(bool)]

        # Re-train the chosen model on the optimal feature set
        final_model = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1) # Or your chosen model
        final_model.fit(X_train_optimal, y_train)
        y_pred_optimal = final_model.predict(X_test_optimal)

        mse_optimal = mean_squared_error(y_test, y_pred_optimal)
        rmse_optimal = np.sqrt(mse_optimal) # Calculate RMSE
        mae_optimal = mean_absolute_error(y_test, y_pred_optimal) # Calculate MAE
        r2_optimal = r2_score(y_test, y_pred_optimal)


        print(f"\nEvaluation with best selected features:")
        print(f"Mean Squared Error (MSE): {mse_optimal:.3f}")
        print(f"Root Mean Squared Error (RMSE): {rmse_optimal:.3f}") # Print RMSE
        print(f"Mean Absolute Error (MAE): {mae_optimal:.3f}") # Print MAE
        print(f"R² Score: {r2_optimal:.3f}")

        # You can also evaluate the accuracy-like metric if desired
        tolerance = 5
        diff_optimal = np.abs(y_test - y_pred_optimal)
        within_range_optimal = np.sum(diff_optimal <= tolerance)
        accuracy_like_optimal = within_range_optimal / len(y_test) * 100
        print(f"Accuracy-like (±{tolerance}): {accuracy_like_optimal:.2f}%")

        # Plot Actual vs Predicted for the final model
        plt.figure(figsize=(8, 5))
        plt.scatter(y_test, y_pred_optimal, alpha=0.5, color='orange')
        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
        plt.xlabel("Actual Critical Temperature")
        plt.ylabel("Predicted Critical Temperature (Optimal Features)")
        plt.title("SVM Regression (GA Features) - Actual vs Predicted")
        plt.grid(True)
        plt.show()

"""**PSO**"""

import numpy as np
# Import SVR for regression
from sklearn.svm import SVR
# Import regression metrics
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# PSO parameters
n_particles = 30       # Number of particles in the swarm
n_iterations = 50      # Number of iterations (generations)
# n_features = X_train.shape[1] # X_train is a pandas DataFrame, get number of columns
# Based on the GA section, it's likely you intended to use the scaled data
# Ensure X_train_scaled is available from previous cells
n_features = X_train_scaled.shape[1]


w = 0.5                # Inertia weight
c1 = 1.5               # Cognitive component (personal best influence)
c2 = 1.5               # Social component (global best influence)

# Set random seed for reproducibility
np.random.seed(SEED)

# Initialize particles and velocities
particles = np.random.rand(n_particles, n_features)      # Positions (values between 0 and 1)
velocities = np.random.rand(n_particles, n_features)     # Velocities

# Initialize personal best positions and scores
personal_best_positions = particles.copy()
# Initialize scores with a value that will be easily beaten (e.g., negative infinity for R² maximization)
personal_best_scores = np.full(n_particles, -np.inf)

# Define a function to calculate the fitness (e.g., R² score)
def calculate_fitness(model, X_train_subset, X_test_subset, y_train, y_test):
    if X_train_subset.shape[1] == 0: # Handle case where no features are selected
        return -np.inf # Return a very low score

    # Ensure the model is re-instantiated or reset if state changes are problematic
    # For SVR, fitting overwrites previous fits, so it's usually fine.
    model.fit(X_train_subset, y_train)
    preds = model.predict(X_test_subset)
    # Use R² score as the fitness metric (higher is better)
    r2 = r2_score(y_test, preds)
    return r2

# --- FIX START ---
# Instantiate the regressor model once before the PSO loop for efficiency
# Using SVR as it's a regressor consistent with other parts of the notebook
pso_model = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)

# Find the initial global best particle by evaluating all initial particles
global_best_score = -np.inf
global_best_position = None

print("Evaluating initial particle fitness...")
for i in range(n_particles):
    # Evaluate initial position
    selected = particles[i] > 0.5

    # Access subsets using numpy indexing on the scaled arrays
    # Ensure X_train_scaled and X_test_scaled are accessible here (they should be if previous cells ran)
    X_train_subset = X_train_scaled[:, selected]
    X_test_subset = X_test_scaled[:, selected]

    score = calculate_fitness(pso_model, X_train_subset, X_test_subset, y_train, y_test)

    # Update personal best (for initial state)
    personal_best_scores[i] = score
    personal_best_positions[i] = particles[i].copy() # Initial personal best is the starting position

    # Update global best if current score is better
    if score > global_best_score:
        global_best_score = score
        global_best_position = particles[i].copy() # Initial global best is the position of the best starting particle

# --- FIX END ---


# PSO main loop
print("\nStarting PSO Feature Selection...")
for iteration in range(n_iterations):
    # Ensure global_best_position is not None before the loop starts
    if global_best_position is None:
         # This case should ideally not be reached with the initialization fix,
         # but as a safeguard, you could potentially re-run initial evaluation
         # or assign a default if necessary. The fix above handles this.
         print("Error: global_best_position is None at start of iteration. Re-evaluating initial population.")
         # Re-evaluate initial population or handle error appropriately
         break # Exit loop if initialization failed unexpectedly


    for i in range(n_particles):
        # Generate random numbers for velocity update
        r1 = np.random.rand(n_features)
        r2 = np.random.rand(n_features)

        # Update velocity based on personal and global bests
        # Now global_best_position is guaranteed to be a numpy array
        velocities[i] = (
            w * velocities[i]
            + c1 * r1 * (personal_best_positions[i] - particles[i])
            + c2 * r2 * (global_best_position - particles[i])
        )

        # Update particle position and clip to [0, 1]
        particles[i] += velocities[i]
        particles[i] = np.clip(particles[i], 0, 1)

        # Evaluate new position
        selected = particles[i] > 0.5

        # Access subsets using numpy indexing on the scaled arrays
        X_train_subset = X_train_scaled[:, selected]
        X_test_subset = X_test_scaled[:, selected]

        # Calculate fitness for the current particle's position
        # Use the pre-instantiated model
        score = calculate_fitness(pso_model, X_train_subset, X_test_subset, y_train, y_test)


        # Update personal best if current score is better
        if score > personal_best_scores[i]:
            personal_best_scores[i] = score
            personal_best_positions[i] = particles[i].copy()

    # Update global best after all particles have been evaluated in this iteration
    current_best_particle_idx = np.argmax(personal_best_scores)
    current_best_particle_score = personal_best_scores[current_best_particle_idx]

    # Check if the best score found in this iteration is better than the overall global best
    if current_best_particle_score > global_best_score:
        global_best_score = current_best_particle_score
        # Update global best position to the position of the particle that achieved the best score in this iteration
        global_best_position = personal_best_positions[current_best_particle_idx].copy()


    print(f"Iteration {iteration+1}/{n_iterations}, Best R²: {global_best_score:.4f}")

# Get the best feature subset from the final global best particle
best_features_mask = global_best_position > 0.5
# Select the columns from the original DataFrame X based on the mask
# X.columns should align with the features used in X_train_scaled
selected_feature_names = X.columns[best_features_mask]
print("\nPSO Feature Selection Completed.")
print("Best features selected by PSO:")
print(selected_feature_names.tolist()) # Print as a list for better readability
print(f"Number of features selected: {np.sum(best_features_mask)}")
print(f"Best R² score achieved during PSO: {global_best_score:.4f}")


# --- Train final model using PSO selected features ---
# Ensure you use the scaled data with the final selected features
X_train_pso_selected = X_train_scaled[:, best_features_mask]
X_test_pso_selected = X_test_scaled[:, best_features_mask]

# Re-train the chosen model (SVR) on the optimal feature set
# Instantiate a new model for the final training
final_model_pso = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1) # Or your chosen regression model

# Check if any features were selected
if X_train_pso_selected.shape[1] == 0:
    print("\nNo features were selected by PSO. Cannot train final model.")
else:
    final_model_pso.fit(X_train_pso_selected, y_train)
    y_pred_pso_optimal = final_model_pso.predict(X_test_pso_selected)

    # Evaluate the final model
    mse_pso = mean_squared_error(y_test, y_pred_pso_optimal)
    rmse_pso = np.sqrt(mse_pso)
    mae_pso = mean_absolute_error(y_test, y_pred_pso_optimal)
    r2_pso = r2_score(y_test, y_pred_pso_optimal)

    print(f"\nEvaluation with PSO-selected features (using SVR):")
    print(f"Mean Squared Error (MSE): {mse_pso:.3f}")
    print(f"Root Mean Squared Error (RMSE): {rmse_pso:.3f}")
    print(f"Mean Absolute Error (MAE): {mae_pso:.3f}")
    print(f"R² Score: {r2_pso:.3f}")

    # You can also evaluate the accuracy-like metric if desired
    tolerance = 5
    diff_pso_optimal = np.abs(y_test - y_pred_pso_optimal)
    within_range_pso_optimal = np.sum(diff_pso_optimal <= tolerance)
    accuracy_like_pso_optimal = within_range_pso_optimal / len(y_test) * 100
    print(f"Accuracy-like (±{tolerance}): {accuracy_like_pso_optimal:.2f}%")

    # Plot Actual vs Predicted for the final model
    plt.figure(figsize=(8, 5))
    plt.scatter(y_test, y_pred_pso_optimal, alpha=0.5, color='darkorange')
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
    plt.xlabel("Actual Critical Temperature")
    plt.ylabel("Predicted Critical Temperature (PSO Features)")
    plt.title("SVM Regression (PSO Features) - Actual vs Predicted")
    plt.grid(True)
    plt.show()